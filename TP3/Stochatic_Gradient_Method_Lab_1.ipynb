{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnjzW0-TPMvF"
   },
   "source": [
    "# __Training of a Neural Network for a classification toy problem.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ie13pHt3gyh8"
   },
   "source": [
    "#### We import standard libraries and the three classes of the tool box ToyNN (ToyPb, nD_data, ToyNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Look at the companion note or to the dedicated notebook for the description of the classes.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random as nprd\n",
    "from matplotlib import pyplot as plt\n",
    "#from matplotlib import cm as cm\n",
    "from toyneuralnetwork import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Woacru-MVEgA"
   },
   "source": [
    "### We  start by choosing a problem ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = ToyPb(name = \"square\", bounds = (-1,1))\n",
    "pb.show_border()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we pick a set of training data and a set of test data that fit the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = 1000\n",
    "DATA = nD_data(n = ndata, pb = pb)\n",
    "\n",
    "ntest = 500\n",
    "TEST = nD_data(n = ntest, pb = pb, init_pred='yes')\n",
    "\n",
    "TEST.show_class()\n",
    "pb.show_border('k--')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We  choose the number layers and the number of nodes by layer for the neural network (with the constaints of   two input nodes and one output node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CardNodes = (2, 4, 6, 4, 1)\n",
    "NN = ToyNN(card = CardNodes, coef_bounds=(-1,1,-1,1), chi=\"tanh\", grid=(-1,1,41))\n",
    "NN.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GafO0zXoJ6Cx"
   },
   "source": [
    "## ___Full Batch Method___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We optmize the coefficents of the neural network (stored in__ NN.W __and__ NN.Bias __) in order to impove its predictions. In practice, we optimize__\n",
    "$$\n",
    "\\dfrac1n\\sum_{\\text{i}=0}^{n-1}\\ell\\left(  y_i \\widehat f(X_i)    \\right),\n",
    "$$\n",
    "__where__ $n=\\text{DATA.n}$, $\\,\\ell=\\text{pb.loss}$, $y_i=\\text{DATA.Y[i]}$,  $X_i=\\text{DATA.X[i]}$ __and where the funcion $\\widehat f(X)=\\text{NN.output}(X)$.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We first implement the full gradient method with fixed step.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AmqPOT9eJ6DC"
   },
   "source": [
    "### _Parameters for the Full Gradient and initialization_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IdVT9w-hJ6DJ"
   },
   "outputs": [],
   "source": [
    "to = 1        # fixed step size of the gradient descent algorithm\n",
    "Nepoch = 200    # Number of epochs \n",
    "\n",
    "plot_period = 20\n",
    "\n",
    "NN = ToyNN(card = CardNodes, coef_bounds=(-1,1,-1,1), chi=\"tanh\", grid=(-1,1,41))\n",
    "\n",
    "cost  = NN.total_loss_and_prediction(pb=pb, DATA=TEST)\n",
    "title = \"Epoch: \" + str(0) + \", Cost: \" + str(cost)\n",
    "print(title)\n",
    "NN.show_pred()\n",
    "TEST.show_class(pred=\"ok\")\n",
    "pb.show_border('w--')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5l_mvC1OJ6Da"
   },
   "source": [
    "### _Full gradient Iterations_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "colab_type": "code",
    "id": "N49qsCBGJ6Df",
    "outputId": "624cf45f-d914-45bf-c277-c73e96fa9773"
   },
   "outputs": [],
   "source": [
    "\n",
    "### Iterations (no sub_iterations)\n",
    "for epoch in range(1, Nepoch + 1):\n",
    "    ## Computation of the descent direction\n",
    "    N=NN.N\n",
    "    # 0-initialization of the descent vectors\n",
    "    NN.init_vector()\n",
    "    # computation and summation over the data of their contributions to the total descent   \n",
    "    for j in range(ndata):\n",
    "        Desc_W, Desc_Bias = NN.descent(X=DATA.X[j], y=DATA.Y[j], pb=pb, tau=to)\n",
    "        NN.add_to_vector(Desc_W, Desc_Bias)\n",
    "    NN.mult_vector(1/ndata)       # renormalization of the sum of descent vectors      \n",
    "        # Update of the parameters\n",
    "    NN.add_vector_to_coefs()\n",
    "\n",
    "\n",
    "#computation of the error (sum of test losses)\n",
    "    if epoch%plot_period==0:\n",
    "        cost = NN.total_loss_and_prediction(DATA=TEST, pb = pb)\n",
    "        title = \"Epoch: \" + str(epoch) + \", Cost: \" + str(cost)\n",
    "        print(title)\n",
    "        NN.show_pred()\n",
    "        TEST.show_class(pred=\"yes\")\n",
    "        pb.show_border('w--')\n",
    "        plt.show()\n",
    "    else:\n",
    "        cost  = NN.total_loss(DATA=TEST, pb=pb)\n",
    "        title = \"Epoch: \" + str(epoch) + \", Cost: \" + str(cost)\n",
    "        print(title)\n",
    "\n",
    "NN.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exexcice : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUlLlKU8ZynE"
   },
   "source": [
    "__1/ Implement the Stochastic Gradient Method with constant step.__\n",
    "\n",
    "\n",
    "__2/ Observe and comment the convergence properties with the full batch metod.__\n",
    "\n",
    "\n",
    "__3/ Implement the Stochastic Gradient Method with decreasing step sizes:__\n",
    "$$\\tau^k := \\dfrac{\\gamma \\tau^0}{\\gamma + k}.$$\n",
    "\n",
    "\n",
    "__4/ Do you observe an improvement? Do you find an empirical method for the choice $\\tau^0$ and $\\gamma$?__\n",
    "\n",
    "__5/ Try the ring problem__ pb = ToyPb(name = \"ring\", bounds = (-1,1)). __What is the behavior of the full batch method on this problem.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "imJTqdW6d4Cx"
   },
   "source": [
    "### _Parameters for the Stochastic Gradient Method and for the Neural network_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xcxR-jiaPMwm"
   },
   "source": [
    "### _Stochastic Gradient Iterations_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = .01         # fixed step size of the gradient descent algorithm\n",
    "Nepoch = 200     # Number of epochs \n",
    "ndata = 1000\n",
    "plot_period = 20\n",
    "\n",
    "CardNodes = (2, 4, 6, 4, 1)\n",
    "NN = ToyNN(card = CardNodes, coef_bounds=(-1,1,-1,1), chi=\"tanh\", grid=(-1,1,41))\n",
    "\n",
    "cost_list = []\n",
    "\n",
    "### Iterations\n",
    "for epoch in range(1, Nepoch + 1):\n",
    "    N=NN.N\n",
    "    # computation and summation over the data of their contributions to the total descent\n",
    "    for _ in range(ndata):\n",
    "        # initialization of the stored descent direction\n",
    "        NN.init_vector()\n",
    "        # selection of a random data to perform the gradient descent on\n",
    "        j = np.random.randint(0, ndata)\n",
    "        # computation of the descent direction\n",
    "        NN.descent(X=DATA.X[j], y=DATA.Y[j], pb=pb, tau=to, add_to_vector=True) \n",
    "        # update of the coefficients\n",
    "        NN.add_vector_to_coefs()\n",
    "\n",
    "#computation of the error (sum of test losses)\n",
    "    if epoch%plot_period==0:\n",
    "        cost = NN.total_loss_and_prediction(DATA=TEST, pb = pb)\n",
    "        title = \"Epoch: \" + str(epoch) + \", Cost: \" + str(cost)\n",
    "        print(title)\n",
    "        NN.show_pred()\n",
    "        TEST.show_class(pred=\"yes\")\n",
    "        pb.show_border('w--')\n",
    "        plt.show()\n",
    "        cost_list.append(cost)\n",
    "    else:\n",
    "        cost  = NN.total_loss(DATA=TEST, pb=pb)\n",
    "        title = \"Epoch: \" + str(epoch) + \", Cost: \" + str(cost)\n",
    "        print(title)\n",
    "        cost_list.append(cost)\n",
    "\n",
    "NN.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cost_list)\n",
    "plt.title(\"Cost evolution with the epochs (stochastic gradient with constant step)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "On observe qu'à nombre d'époques équivalent, la prédiction semble converger vers la solution (le carré) beaucoup plus rapidement avec la méthode du gradient stochastique, qu'avec la méthode du gradient \"full-batch\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "### _Stochastic Gradient with decreasing step sizes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_decreasing_step(tau0, gamma):\n",
    "\n",
    "    Nepoch = 200\n",
    "    ndata = 1000\n",
    "    plot_period = 20\n",
    "\n",
    "    CardNodes = (2, 4, 6, 4, 1)\n",
    "    NN = ToyNN(card=CardNodes, coef_bounds=(-1, 1, -1, 1), chi=\"tanh\", grid=(-1, 1, 41))\n",
    "\n",
    "    cost_list = []\n",
    "    \n",
    "    for epoch in range(1, Nepoch + 1):\n",
    "        ## Computation of the descent direction\n",
    "\n",
    "        for k in range(ndata):\n",
    "            j = np.random.randint(0, ndata)  # Selection of random data\n",
    "            tau = gamma * tau0 / (gamma + (epoch-1)*ndata + k) # (epoch-1)*ndata + k is the current iteration number\n",
    "            NN.init_vector()  # 0-initialization of the descent vectors\n",
    "            Desc_W, Desc_Bias = NN.descent(\n",
    "                X=DATA.X[j], y=DATA.Y[j], pb=pb, tau=tau)\n",
    "            NN.add_vector_to_coefs(DW=Desc_W, DBias=Desc_Bias)\n",
    "\n",
    "        #computation of the error (sum of test losses)\n",
    "        if epoch % plot_period == 0:\n",
    "            cost = NN.total_loss_and_prediction(DATA=TEST, pb=pb)\n",
    "            title = \"Epoch: \" + str(epoch) + \", Cost: \" + str(cost)\n",
    "            print(title)\n",
    "            NN.show_pred()\n",
    "            TEST.show_class(pred=\"yes\")\n",
    "            pb.show_border('w--')\n",
    "            plt.show()\n",
    "            cost_list.append(cost)\n",
    "        else:\n",
    "            cost = NN.total_loss(DATA=TEST, pb=pb)\n",
    "            title = \"Epoch: \" + str(epoch) + \", Cost: \" + str(cost)\n",
    "            print(title)\n",
    "            cost_list.append(cost)\n",
    "            \n",
    "    NN.show()\n",
    "    plt.figure()\n",
    "    plt.plot(cost_list)\n",
    "    plt.title(\"Cost evolution with the epochs (stochastic gradient with decreasing step)\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_gradient_decreasing_step(0.1,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "On observe que l'erreur asymptotique n'est pas nécessairement meilleure, mais le bruit est grandement diminué avec la méthode du pas décroissant. \n",
    "\n",
    "Pour les choix des paramètres :\n",
    "* On prend un $\\tau^0$ plus grand que pour le gradient stochastique classique, puisque celui-ci va diminuer au cours des itérations.\n",
    "* On augmente progressivement $\\gamma$ jusqu'à trouver un bon compromis entre vitesse de convergence et diminution du bruit.\n",
    "\n",
    "Après avoir appliqué cette méthode empirique, on obtient des résultats satisfaisants avec $\\tau^0 = 0.1$ et $\\gamma = 1000$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = ToyPb(name = \"ring\", bounds = (-1,1))\n",
    "pb.show_border()\n",
    "\n",
    "ndata = 1000\n",
    "DATA = nD_data(n = ndata, pb = pb)\n",
    "\n",
    "ntest = 500\n",
    "TEST = nD_data(n = ntest, pb = pb, init_pred='yes')\n",
    "\n",
    "TEST.show_class()\n",
    "pb.show_border('k--')\n",
    "plt.legend(loc=1)\n",
    "plt.show()\n",
    "\n",
    "CardNodes = (2, 4, 6, 4, 1)\n",
    "NN = ToyNN(card = CardNodes, coef_bounds=(-1,1,-1,1), chi=\"tanh\", grid=(-1,1,41))\n",
    "NN.show()\n",
    "\n",
    "to = 1        # fixed step size of the gradient descent algorithm\n",
    "Nepoch = 200    # Number of epochs \n",
    "\n",
    "plot_period = 20\n",
    "\n",
    "NN = ToyNN(card = CardNodes, coef_bounds=(-1,1,-1,1), chi=\"tanh\", grid=(-1,1,41))\n",
    "\n",
    "cost  = NN.total_loss_and_prediction(pb=pb, DATA=TEST)\n",
    "title = \"Epoch: \" + str(0) + \", Cost: \" + str(cost)\n",
    "print(title)\n",
    "NN.show_pred()\n",
    "TEST.show_class(pred=\"ok\")\n",
    "pb.show_border('w--')\n",
    "plt.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Iterations (no sub_iterations)\n",
    "for epoch in range(1, Nepoch + 1):\n",
    "    ## Computation of the descent direction\n",
    "    N=NN.N\n",
    "    # 0-initialization of the descent vectors\n",
    "    NN.init_vector()\n",
    "    # computation and summation over the data of their contributions to the total descent   \n",
    "    for j in range(ndata):\n",
    "        Desc_W, Desc_Bias = NN.descent(X=DATA.X[j], y=DATA.Y[j], pb=pb, tau=to)\n",
    "        NN.add_to_vector(Desc_W, Desc_Bias)\n",
    "    NN.mult_vector(1/ndata)       # renormalization of the sum of descent vectors      \n",
    "        # Update of the parameters\n",
    "    NN.add_vector_to_coefs()\n",
    "\n",
    "\n",
    "#computation of the error (sum of test losses)\n",
    "    if epoch%plot_period==0:\n",
    "        cost = NN.total_loss_and_prediction(DATA=TEST, pb = pb)\n",
    "        title = \"Epoch: \" + str(epoch) + \", Cost: \" + str(cost)\n",
    "        print(title)\n",
    "        NN.show_pred()\n",
    "        TEST.show_class(pred=\"yes\")\n",
    "        pb.show_border('w--')\n",
    "        plt.show()\n",
    "    else:\n",
    "        cost  = NN.total_loss(DATA=TEST, pb=pb)\n",
    "        title = \"Epoch: \" + str(epoch) + \", Cost: \" + str(cost)\n",
    "        print(title)\n",
    "\n",
    "NN.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Il semble que la méthode de gradient full-batch soit incapable d'isoler l'anneau: lors du calcul du gradient sur l'ensemble des points, le gradient des points à l'intérieur et à l'extérieur de l'anneau \"se compensent\", ce qui rend son isolement impossible en observant le gradient global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_gradient_decreasing_step(0.1,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En revanche, avec la méthode de gradient stochastique, les prédictions convergent rapidement vers la solution. Le problème évoqué auparavant n'existe pas avec des gradients \"locaux\"."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "GafO0zXoJ6Cx",
    "5l_mvC1OJ6Da",
    "ZzS5-IzwaKn3",
    "89AjhkJ2aKoB"
   ],
   "name": "ToyNN_class.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "8f647a481d9333e43257dc41057b6fd63708760adb5033c3e0c71c25cbee6d33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('sdia-optimisation': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
