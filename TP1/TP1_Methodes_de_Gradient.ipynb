{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# L'algorithme de descente de gradient "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Le cas d'une fonctionnelle quadratique\n",
    " \n",
    "Tout d'abord, nous appliquons la méthode à une fonctionnelle quadratique :\n",
    "\\begin{align}\\label{quadf}\\tag{1} f(x)=\\dfrac12 x^TA x + b^T x +c,\\qquad \\text{pour } x\\in \\mathbb{R}^ N, \\end{align}\n",
    "avec\n",
    "$A$ une matrice réelle $N\\times N$, symétrique définie positive, $b\\in\\mathbb{R}^N$ et $c\\in \\mathbb{R}$. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 1.** Calculez le gradient et la matrice Hessienne de $f$.\n",
    "\n",
    "**Question 2.** Que pouvez-vous en déduire sur la nature de $f$ ?\n",
    "\n",
    "**Question 3.** Montrez que $f$ atteint son minimum sur $\\mathbb{R}^N$ en un seul point $x^*$. Donnez une caractérisation de ce point. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 L'algorithme de descente de gradient à pas optimal\n",
    "\n",
    "Soit $f$ une fonction convexe et coercive de classe $C^1$ sur $\\mathbb{R}^N$. L'algorithme optimal de descente de gradient est défini comme suit. \n",
    "\n",
    "Soit $x^0\\in \\mathbb{R}^N$ (on essaie de choisir $x^0$ proche de $x^*$, en l'absence d'indication alors on prend $x^0=0$). \n",
    "\n",
    "Ensuite, pour $k=0,1,2,\\ldots\\ $ jusqu'à convergence, répéter : \n",
    "\n",
    "$$\n",
    "\\left|\n",
    "\\begin{array}{lcl}\n",
    "d^k& \\longleftarrow & -\\nabla f(x^k),\\\\\n",
    "\\tau^k &\\longleftarrow &\\mathop{argmin}_{t>0} f(x^k + td^k),\\\\ \n",
    "x^{k+1}&\\longleftarrow &x^k+\\tau^k d^k\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 4.** Proposez un critère d'arrêt pour l'algorithme qui utilise la caractérisation de la question **3**. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Remarque :** En général, on ne sait pas calculer $\\tau^k$ et en pratique, la deuxième étape est remplacée par une recherche approchée. Cependant, lorsque $f$ est quadratique, le calcul de $\\tau^k$ est facile. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 5.** Dans le cas de la fonction quadratique (1), explicitez $d^k$ et $\\tau^k$ comme fonctions de $A$, $x^k$ et $b$. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Maintenant, nous spécifions $N=2$ et\n",
    "$$ A=\\binom{C\\quad 0}{0\\quad 1},\\quad C\\ge 1,\\quad b=0,\\quad c=0.$$\n",
    "**Question 6.** Quel est l'infimum de $f$ sur $\\mathbb{R}^2$ dans ce cas ? Donner $x^*$. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 7.** Deux fonctions sont données ci-dessous :\n",
    "- une fonction qui dessine un champ de vecteur donné par une application $F$. À titre d'exemple, il est appliqué à l'application $G(x,y)=(x, 25y)$.\n",
    "- une fonction qui dessine quelques lignes de niveau d'une fonction $f$. Il est appliqué à $g(x,y)=\\dfrac{x^2+25y^2}2$. Notez que $G=\\nabla g$. Qu'observez-vous ? "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt \r\n",
    "\r\n",
    "def draw_vector_field(F, xmin, xmax, ymin, ymax, N=15):\r\n",
    "    X = np.linspace(xmin, xmax, N)  # x coordinates of the grid points\r\n",
    "    Y = np.linspace(ymin, ymax, N)  # y coordinates of the grid points\r\n",
    "    U, V = F(*np.meshgrid(X, Y))  # vector field\r\n",
    "    M = np.hypot(U, V)  # compute the norm of (U,V)\r\n",
    "    M[M == 0] = 1  # avoid division by 0\r\n",
    "    U /= M  # normalize the u componant\r\n",
    "    V /= M  # normalize the v componant\r\n",
    "    return plt.quiver(X, Y, U, V, angles='xy')\r\n",
    "\r\n",
    "def level_lines(f, xmin, xmax, ymin, ymax, levels, N=500):\r\n",
    "    x = np.linspace(xmin, xmax, N)\r\n",
    "    y = np.linspace(ymin, ymax, N)\r\n",
    "    z = f(*np.meshgrid(x, y))\r\n",
    "    level_l = plt.contour(x, y, z, levels=levels)\r\n",
    "    #plt.clabel(level_l, levels, fmt='%.1f') \r\n",
    "\r\n",
    "\r\n",
    "g = lambda x, y: .5*(x**2 + 12*y**2)\r\n",
    "G = lambda x, y: np.array([x, 12*y])\r\n",
    "%matplotlib inline\r\n",
    "plt.figure(figsize=(12,6))\r\n",
    "level_lines(g, -8, 8, -2.1, 2.1, np.linspace(0, 25, 5))\r\n",
    "draw_vector_field(G,  -8, 8, -2.1, 2.1, 10)\r\n",
    "plt.axis('equal')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observations\n",
    "* L'intensité des lignes de niveau et la direction du gradient montrent qu'un minimum global de g est atteint au point (0,0).\n",
    "* On peut également observer que g croît beaucoup plus rapidement selon y que selon x (facteur 25), ce qui se traduit par des lignes de niveau \"aplaties\" selon x, et des flèches de gradient plus rapprochées selon y."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 8.** Implémentez l'algorithme de descente de gradient à pas optimal. Le point initial doit être $x^0=\\binom1C$.\n",
    "\n",
    "**Question 9.** Sur le même graphique, représentez les itérations, quelques lignes de niveau de $f$ et le champ de vecteur normalisé $\\dfrac {1}{|\\nabla f|}\\nabla f$. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Question 8\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt \r\n",
    "\r\n",
    "f = lambda x, C: .5*(C*x[0]**2 + x[1]**2)\r\n",
    "F = lambda x, C: np.array([C*x[0], x[1]])\r\n",
    "x0 = lambda C: np.array([1, C])\r\n",
    "\r\n",
    "norm = lambda A: np.linalg.norm(A)\r\n",
    "tauOpt = lambda x, C: (C**2 * x[0]**2 + x[1]**2) / (C**3 * x[0]**2 + x[1]**2)\r\n",
    "\r\n",
    "\r\n",
    "def gradient_descent(C, tol=1e-8):\r\n",
    "    fc = lambda x: f(x, C)\r\n",
    "    Fc = lambda x: F(x, C)\r\n",
    "    x = x0(C)\r\n",
    "    epsilon = tol * norm(Fc(x0(C)))\r\n",
    "    plt.plot(x[0], x[1], 'g*')\r\n",
    "    counter = 0\r\n",
    "    while(norm(Fc(x)) >= epsilon):\r\n",
    "        counter += 1\r\n",
    "        d = - Fc(x)\r\n",
    "        tau = tauOpt(x,C)\r\n",
    "        # tau = 1e-2\r\n",
    "        x = x + tau*d\r\n",
    "        plt.plot(x[0], x[1], 'ro')\r\n",
    "    plt.plot(x[0], x[1], 'g')\r\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Question 9\n",
    "\n",
    "def plot_gradient(C) :\n",
    "    f_plot = lambda x, y, C: C*x**2 + y**2\n",
    "    F_plot = lambda x, y, C: np.array([2*C*x, 2*y])\n",
    "\n",
    "    fc_plot = lambda x, y: f_plot(x, y, C)\n",
    "    Fc_plot = lambda x, y: F_plot(x, y, C)\n",
    "\n",
    "    %matplotlib inline\n",
    "    plt.figure(figsize=(12,6))\n",
    "    level_lines(fc_plot, -1*C, 1*C, -2*C, 2*C, np.linspace(0, 25, 15))\n",
    "    draw_vector_field(Fc_plot,  -1*C, 1*C, -2*C, 2*C, 10)\n",
    "\n",
    "    gradient_descent(C)\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    \n",
    "plot_gradient(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 10.** Changer la valeur de $C$ de 1 à 32 ($C=1,2,4,8,16,32$). Qu'observez-vous ?\n",
    "\n",
    "**Question 11.** Tracez le nombre d'itérations de la méthode en fonction de $C$. Faites une hypothèse. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Question 10\n",
    "\n",
    "Cs = [1, 2, 4, 8, 16, 32]\n",
    "for C in Cs:\n",
    "    plot_gradient(C)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Question 11\n",
    "\n",
    "# Problème avec l'expression de tau"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Le cas d'une fonction convexe régulière, line search. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On considère la fonction définie par\n",
    "$$\n",
    "f(x,y):= \\cosh(x) + \\sin^2(x+y),\\qquad \\text{pour }z=(x,y)\\in \\mathbb{R}^2.\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 12.** Montrer que $f$ est convexe au voisinage de $z^0_*$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons appliquer un algorithme de descente de gradient avec ``line search'' à la fonction $f$. Plus précisément :\n",
    "\n",
    "Étant donné  $z^0=(x^0,y^0)\\in\\mathbb{R}^2$, calculer de manière récursive, jusqu'à convergence,\n",
    "\n",
    "$$\n",
    "\\left|\n",
    "\\begin{array}{lcl}\n",
    "d^k& \\longleftarrow & -\\nabla f(z^k),\\\\\n",
    "\\tau^k &\\longleftarrow & \\text{Line-search}\\ \\left(\\ t\\mapsto f(z^k + td^k)\\ \\right),\\\\ \n",
    "z^{k+1}&\\longleftarrow &z^k+\\tau^k d^k\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Précisons la deuxième étape. On remarque d'abord que pour $t>0$,\n",
    "\n",
    "$$\n",
    "f(z^k+t d^k) \\,=\\, f(z^k) -t \\|d^k\\|^2 +o(t).\n",
    "$$\n",
    "\n",
    "En fait, si $f$ est convexe au voisinage de $z^k$, on a aussi pour $t>0$ assez petit, \n",
    "\n",
    "$$\n",
    "f(z^k+t d^k)\\, \\ge\\, f(z^k) -t \\|d^k\\|^2,\n",
    "$$\n",
    "\n",
    "donc on ne peut pas demander $f(z^k+t d^k) \\,\\le\\, f(z^k) -t \\|d^k\\|^2$. \n",
    "\n",
    "L'idée de la *condition Armijo* est de demander un peu moins. Fixons un $\\alpha\\in (0,1)$ : la condition Armijo s'écrit : \n",
    "\n",
    "$$\n",
    "\\tag{2}f(z^k+t d^k)\\, \\le\\, f(z^k) -\\alpha\\, t \\|d^k\\|^2.\n",
    "$$\n",
    "\n",
    "En utilisant le développement limité ci dessus, on a \n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "f(z^k+t d^k) &=& f(z^k) -t \\|d^k\\|^2 +o(t)\\\\\n",
    "   &=& f(z^k) -\\alpha\\, t \\|d^k\\|^2 - (1-\\alpha)t\\|d^k\\|^2 +o(t)\\\\\n",
    "   & = & f(z^k) -\\alpha\\, t \\|d^k\\|^2 -t \\left[(1-\\alpha)\\|d^k\\|^2 +o(1)\\right]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Pour $t>0$ assez petit, le terme entre crochet est positif et donc (2) est vrai.\n",
    "\n",
    "Nous ne voulons pourtant pas choisir un $\\tau^k$ trop petit (l'algorithme calerait). Pour éviter cela, nous fixons un pas maximal $\\tau_0$ et un facteur $\\beta\\in(0,1)$ et nous testons successivement (2) avec $t=\\tau_0$, $t=\\tau_0\\beta$, $t=\\tau_0\\beta^2$, ... \n",
    "\n",
    "On choisi $\\tau^k=\\tau_0\\beta^j$ où $j$ est le premier entier tel que $t=\\tau_0\\beta^j$ vérifie (2).\n",
    "\n",
    "Remarquez que comme $0<\\beta<1$, cet entier existe. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 13.** Implémentez la méthode ci-dessus, avec $\\alpha=0.5$, $\\beta=0.75$. Commencez par $z^0=(1,0.5)$ et $\\tau_0=1$. Ensuite, pour $k\\ge 1$ utilisez $\\tau_0\\leftarrow\\tau^0/\\beta$.\n",
    "\n",
    "Tout d'abord pour vous aider, la cellule suivante montre quelques ensembles de niveaux de $f$ et le champ de vecteur normalisé $\\dfrac {1}{|\\nabla f|}\\nabla f$ au voisinage de $z^*$. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def draw_vector_field_2(F, xmin, xmax, ymin, ymax, N=15):\n",
    "    X = np.linspace(xmin, xmax, N)  # x coordinates of the grid points\n",
    "    Y = np.linspace(ymin, ymax, N)  # y coordinates of the grid points\n",
    "    U, V = F(*np.meshgrid(X, Y))  # vector field\n",
    "    M = np.hypot(U, V)  # compute the norm of (U,V)\n",
    "    M[M == 0] = 1  # avoid division by 0\n",
    "    U /= M  # normalize the u componant\n",
    "    V /= M  # normalize the v componant\n",
    "    return plt.quiver(X, Y, U, V, angles='xy')\n",
    "\n",
    "def level_lines_2(f, xmin, xmax, ymin, ymax, levels, N=500):\n",
    "    x = np.linspace(xmin, xmax, N)\n",
    "    y = np.linspace(ymin, ymax, N)\n",
    "    z = f(*np.meshgrid(x, y))\n",
    "    level_l = plt.contour(x, y, z, levels=levels)\n",
    "    #plt.clabel(level_l, levels, fmt='%.1f') \n",
    "\n",
    "f = lambda x, y : np.cosh(x)+ np.sin(x + y)**2\n",
    "df = lambda x, y : np.array([np.sinh(x) + 2*np.cos(x + y)*np.sin(x + y), 2*np.cos(x + y)*np.sin(x + y)])\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(9,6))\n",
    "level_lines_2(f, -1.1, 1.1, -1.1, 1.1, np.linspace(1, 3, 10))\n",
    "draw_vector_field_2(df, -1.1, 1.1, -1.1, 1.1, 10)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Question 13\n",
    "\n",
    "# ls -> line search\n",
    "# alpha ?\n",
    "def gradient_descent_ls(f, F, alpha, beta, z0, tau0, tol=1e-8, max_iterations = 100):\n",
    "    counter = 0\n",
    "    x = z0[0]\n",
    "    y = z0[1]\n",
    "    while(norm(F(x,y)) >= tol * norm(F(z0[0], z0[1])) and counter <= max_iterations):\n",
    "        counter += 1\n",
    "        d = -df(x,y)\n",
    "        if(counter == 1): \n",
    "            tau = tau0\n",
    "        else : tau = tau*beta\n",
    "        x = x + tau*d[0]\n",
    "        y = y + tau*d[1]\n",
    "        plt.plot(x, y, 'ro')\n",
    "    plt.plot(x, y, 'g')\n",
    "    z = np.array([x,y])\n",
    "    return z"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(9,6))\n",
    "level_lines_2(f, -1.1, 1.1, -1.1, 1.1, np.linspace(1, 3, 10))\n",
    "draw_vector_field_2(df, -1.1, 1.1, -1.1, 1.1, 10)\n",
    "\n",
    "gradient_descent_ls(f, df, 0.5, 0.75, np.array([0,0.5]), 1)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On considère maintenant la fonction définie sur $\\mathbb{R}^3$ par \n",
    "$$\n",
    "f(x,y,z):= \\cosh(x) + \\sin^2(x+y) + (y-z)^2,\\qquad \\text{pour }w=(x,y,z)\\in \\mathbb{R}^2.\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question 14.** Appliquez la méthode d'optimisation ci-dessus à cette fonction, en commençant par $w^0=(1,0.5,1)$. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Pour les plots en 3 dimensions\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# exemple\n",
    "t = np.linspace(0,np.pi,101)\n",
    "x, y, z = np.cos(t), np.sin(t), t+.5*np.sin(t)**2\n",
    "\n",
    "ax = Axes3D(plt.figure())  # Define the 3D plot\n",
    "ax.set(xlabel=r'$x$', ylabel=r'$y$', zlabel=r'$z$')\n",
    "ax.plot(x, y, z,'.')  # Plot of the trajectory\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Solution"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('sdia-python': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "interpreter": {
   "hash": "47a3d19953d0682b2fed03a7de16340ad054781b6349bd7c1cab4a5c5105088a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}