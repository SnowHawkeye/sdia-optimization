{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1 align=\"center\"><font size=\"6\"> The gradient descent algorithm </font> (second part)</h1>\n",
    "<hr> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Table of contents</h1>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ul>\n",
    "        <li><a href=\"#prelim\">Preliminaries</a></li>\n",
    "        <li><a href=\"#Newton\">The Newton method</a></li>\n",
    "        <li><a href=\"#BFGS\">A Quasi-Newton Meton (BFGS)</a></li>\n",
    "    </ul>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='prelim'></a>\n",
    "<h2>Preliminaries</h2>\n",
    "<hr>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we import the required libraries. We define two functions used to visualize *1/* the level lines of the objective function and *2/* their gradient vector fields (when they depend on 2 variables). There is also two examples of plots used to observe the convergence of the optimization methods."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from ipywidgets import interact\r\n",
    "from mpl_toolkits.mplot3d import Axes3D\r\n",
    "\r\n",
    "def draw_vector_field(F, xmin, xmax, ymin, ymax, N=15):\r\n",
    "    X = np.linspace(xmin, xmax, N)  # x coordinates of the grid points\r\n",
    "    Y = np.linspace(ymin, ymax, N)  # y coordinates of the grid points\r\n",
    "    U, V = F(*np.meshgrid(X, Y))  # vector field\r\n",
    "    M = np.hypot(U, V)  # compute the norm of (U,V)\r\n",
    "    M[M == 0] = 1  # avoid division by 0\r\n",
    "    U /= M  # normalize the u componant\r\n",
    "    V /= M  # normalize the v componant\r\n",
    "    return plt.quiver(X, Y, U, V, angles='xy')\r\n",
    "\r\n",
    "def level_lines(f, xmin, xmax, ymin, ymax, levels, N=500):\r\n",
    "    x = np.linspace(xmin, xmax, N)\r\n",
    "    y = np.linspace(ymin, ymax, N)\r\n",
    "    z = f(*np.meshgrid(x, y))\r\n",
    "    level_l = plt.contour(x, y, z, levels=levels)\r\n",
    "    #plt.clabel(level_l, levels, fmt='%.1f') \r\n",
    "\r\n",
    "f = lambda x, y : np.cosh(x)+ np.sin(x + y)**2\r\n",
    "df = lambda x, y : np.array([np.sinh(x) + 2*np.cos(x + y)*np.sin(x + y), 2*np.cos(x + y)*np.sin(x + y)])\r\n",
    "%matplotlib inline\r\n",
    "level_lines(f, -1.1, 1.1, -1.1, 1.1, np.linspace(1, 3, 10))\r\n",
    "draw_vector_field(df, -1.1, 1.1, -1.1, 1.1, 10)\r\n",
    "plt.axis('equal')\r\n",
    "plt.show()\r\n",
    "\r\n",
    "\r\n",
    "%matplotlib inline\r\n",
    "N=100\r\n",
    "t = np.linspace(0,2*np.pi,N, endpoint='False')\r\n",
    "x, y, z = np.cos(t), np.sin(t), np.sin(3*t+np.pi/4)\r\n",
    "ax = Axes3D(plt.figure())  # Define the 3D plot\r\n",
    "ax.set(xlabel=r'$x$', ylabel=r'$y$', zlabel=r'$z$')\r\n",
    "ax.plot(x, y, z,'.')  # Plot of the trajectory\r\n",
    "plt.show()\r\n",
    "\r\n",
    "\r\n",
    "# plot of the values of f along the iterations.\r\n",
    "N = 10\r\n",
    "F = 2**(-np.linspace(0,N,N+1))\r\n",
    "plt.figure()\r\n",
    "plt.semilogy(range(N + 1), F, '.', linestyle='dashed')\r\n",
    "\r\n",
    "len(t)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='Newton'></a>\n",
    "<h2>The Newton method</h2>\n",
    "<hr>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Newton (or Newton-Raphson) method is an iterative descent method where the descent direction at step $k$ is chosen in order to minimize the second order Taylor expansion of $f$ around the current point $x^k$, namely,\r\n",
    "$$\r\n",
    "\\tag{4}\r\n",
    "m_k(d):=f(x^k) + d\\cdot \\nabla f(x^k) + \\dfrac12 d^T D^2 f(x^k) d.\r\n",
    "$$\r\n",
    "If the matrix $D^2 f(x^k)$ is invertible (in particular if it is positive definite) the minimizer of $m^k$ exists and is unique. We note $H^k$ the inverse of $D^2 f(x^k)$, $g^k:=\\nabla f(x^k)$ and $d^k$ the minimizer of (4)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Question 13.*** Express $d^k$ as a function of $H^k$ and $g^k$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___Solution:___ \r\n",
    "\r\n",
    "$\\nabla m_k(d) = D^2 f(x^k)d + \\nabla f(x^k)$ (cf. TP1) s'annule pour $d^k = -H^kg^k$, qui minimise donc $m_k(d)$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When the descent direction $d^k$ has been computed, we proceed as in the gradient descent method and use a line-search method to find $t$ such that $f(x^k+ t d^k)$ is sufficienlty smaller than $f(x^k)$. We will use again the Armijo criterion:  \n",
    "\n",
    "$$\n",
    "\\tag{5} \n",
    "f(x^k+t d^k)\\, \\le\\, f(x^k) + \\alpha\\, t \\left<d^k;g^k\\right>.\n",
    "$$\n",
    "\n",
    "Then the Newton-Raphson algorithm with Armijo criterion runs as follows: Fix $\\alpha \\in (0,1)$ and pick some $x^0\\in {\\mathbb R}^N$. Then for $k=0,1,\\ldots$, up to convergence, repeat: \n",
    "$$\n",
    "\\left|\n",
    "\\begin{array}{l}\n",
    "\\text{Compute }d^k,\\\\\n",
    "\\text{Find }\\tau^k>0 \\text{ such that (5) holds true for }t=\\tau^k,\\\\\n",
    "\\text{Set }x^{k+1}:= x^k + \\tau^k d^k.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "An important point here is that, when $x^k$ is sufficiently close to a local minimizer $x^*$ of $f$ which is such that $D^2f(x^*)$ is positive definite, then the choice $\\tau^k=1$ provides a quadratic convergence to $x^*$, _i.e._ \n",
    "$$\n",
    "\\|x^{k+1}-x^*\\|\\leq C \\|x^k - x^*\\|^2.\n",
    "$$\n",
    "If we don't want to loose this super-linear behavior, we need to pick $\\tau^k=1$. To achieve this, we start the back-tracking iterations with $\\tau^k_0=1$ and we choose a small parameter in (5) ($\\alpha=0.1$ for instance).\n",
    "<br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let $\\Lambda>0$. We define $f_\\Lambda(x,y):=(1-x)^2 + \\Lambda\\,(y-x^2)^2$, for $(x,y)\\in\\mathbb{R}^2$.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Question 14.__ Compute $\\nabla f_\\Lambda(x,y)$. Find the minimizer(s) of $f_\\Lambda$. Plot some level lines of $f_\\Lambda$ together with the renormalized vector field $(1/|\\nabla f_\\Lambda|)\\nabla f_\\Lambda$ for $\\Lambda=100$. Compute $D^2 f(x,y)$ and its inverse $H_\\Lambda(x,y)$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Lambda = 1000\r\n",
    "f = lambda x,y : ( x - 1)**2 + Lambda*(y - x**2)**2\r\n",
    "df = lambda x,y : np.array(\r\n",
    "    [\r\n",
    "        2*(x-1 + 2*Lambda*x*(x**2 - y)),\r\n",
    "        2*Lambda * (y - x**2)                    \r\n",
    "                            ])\r\n",
    "ddf = lambda x,y : np.array([\r\n",
    "    [2-4*Lambda*(y-3*x**2), -4*Lambda*x],\r\n",
    "    [-4*Lambda*x          ,  2*Lambda  ]\r\n",
    "    ])\r\n",
    "HH = lambda x,y : 1/(4*Lambda*(1+2*Lambda*(x**2-y))) * np.array([\r\n",
    "    [2*Lambda  , 4*Lambda*x              ],\r\n",
    "    [4*Lambda*x,  2-4*Lambda*(y-3*x**2)  ]\r\n",
    "    ])\r\n",
    "\r\n",
    "level_lines(f, .8, 1.2, 0.8, 1.2, np.linspace(0, 30, 80))\r\n",
    "draw_vector_field(df, .8, 1.2, 0.8, 1.2, 15)\r\n",
    "plt.plot(1,1,'or')\r\n",
    "plt.axis('equal')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Question 15.__ Implement the Newton method and apply it to the above function with $\\alpha = 0.1$, $\\beta=0.75$ and $x^0=(0,0)$. Represent the iterations on a graph and plot $\\ \\log(f_\\Lambda(x^k))\\ $ as a function of $k$. Observe and comment.\n",
    "\n",
    "_Hint:_ First test the algorithm on the quadratic function below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# For the test\r\n",
    "'''\r\n",
    "f = lambda x,y : ( x - 1)**2 + 2*(y - 1)**2\r\n",
    "df = lambda x,y : np.array([2*(x - 1) , 4*(y - 1)])\r\n",
    "ddf = lambda x,y : np.array([[2  , 0], [0, 2]])\r\n",
    "HH = lambda x,y : np.array([[.5, 0], [0, .25]])\r\n",
    "'''\r\n",
    "\r\n",
    "pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Armijo criterion\r\n",
    "def armijoCriterion(f, x, t, d, g, alpha,):\r\n",
    "    return f(x + t*d) <= f(x) + alpha*t*np.dot(d, g)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Parameters\r\n",
    "alpha, beta = .1, .75\r\n",
    "epsilon = 1e-8\r\n",
    "itermax = 200\r\n",
    "iter_ls_max = 40\r\n",
    "\r\n",
    "## initialization \r\n",
    "x0 = np.array([0,0])\r\n",
    "max_norm = np.linalg.norm(df(x0[0], x0[1])*epsilon)\r\n",
    "x = x0\r\n",
    "iter = 0\r\n",
    "iter_ls = 0\r\n",
    "W = []\r\n",
    "\r\n",
    "print(np.shape(HH(x[0],x[1])))\r\n",
    "\r\n",
    "## Optimization loop\r\n",
    "while (np.linalg.norm(df(x[0], x[1])) >= max_norm or iter <= itermax):\r\n",
    "    iter += 1\r\n",
    "    \r\n",
    "    # compute d\r\n",
    "    d = - HH(x[0],x[1]).dot(df(x[0],x[1]))\r\n",
    "    \r\n",
    "    # find t\r\n",
    "    t = 1\r\n",
    "    while(iter_ls <= iter_ls_max):\r\n",
    "        iter_ls += 1\r\n",
    "        if f(x[0] + t*d[0], x[1] + t*d[1]) <= f(x[0], x[1]) + alpha*t*np.dot(d, df(x[0], x[1])): break\r\n",
    "        else: t = t*alpha\r\n",
    "        \r\n",
    "    # add x to the list of results and compute the next one\r\n",
    "    W.append(np.array([x[0], x[1]]))\r\n",
    "    x = x + t*d\r\n",
    "\r\n",
    "W = np.array(W)\r\n",
    "\r\n",
    "# plot the results \r\n",
    "plt.figure()\r\n",
    "plt.plot(W[:,0],W[:,1],'.',linestyle='-')\r\n",
    "level_lines(f, 0, 2, 0, 2, np.linspace(1, 3, 10))\r\n",
    "draw_vector_field(df, 0 , 2, 0, 2, 10)\r\n",
    "plt.axis('equal')\r\n",
    "plt.show()\r\n",
    "\r\n",
    "# plot of the values of f along the iterations.\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure()\r\n",
    "\r\n",
    "number_of_k = np.shape(W)[0]\r\n",
    "k_list = [k for k in range(number_of_k)]\r\n",
    "log_f = [np.log(f(x[0], x[1])) for x in W]\r\n",
    "\r\n",
    "\r\n",
    "plt.plot(k_list,log_f,'.',linestyle='-')\r\n",
    "plt.axis('equal')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='BFGS'></a>\n",
    "<h2> A Quasi-Newton Meton (BFGS)</h2>\n",
    "<hr>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When the number of parameters is large as it is usual in Machine Learning, computing the Hessian matrices $D^2f(x^k)$  and solving the linear systems $D^2f(x^k) d^k=-g^k$ may be too costly. However it is often still possible to achieve superlinear convergence by replacing $[D^2f(x^k)]^{-1}$ by a cheap approximation $H^k$.  There exist several algorithms based on this idea. We present one of the most popular: the BFGS method named after their discoverers (Broyden, Fletcher, Goldfarb and Shanno). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us explain the method. Assume that at step $k$ we have symmetric positive definite approximation $H^k$ of $\\left[D^2f(x^k)\\right]^{-1}$. We note $B^k$ its inverse (which is an approximation of $D^2f(x^k)$). As above, we define our descent direction $d^k$ as the minimizer of \n",
    "$$\n",
    "f(x^k) + d\\cdot \\nabla f(x^k) + \\dfrac12 d^T B^k d.\n",
    "$$\n",
    "This leads to\n",
    "$$\n",
    "d^k = -\\left[B^k\\right]^{-1} \\nabla f(x^k) = - H^k g^k. \n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we find $\\tau^k$ satisfying (5) and we set \n",
    "$$\n",
    "x^{k+1} := x^k +\\tau^k d^k.\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we need an approximation $H^{k+1}$ of $\\left[D^2f(x^{k+1})\\right]^{-1}$. For this, let us recall that we want \n",
    "$$\n",
    "\\tilde m_{k+1} (d):= f(x^{k+1}) + g^{k+1}\\cdot d +\\dfrac 12 d^T B^{k+1} d,\n",
    "$$\n",
    "to be an approximation of \n",
    "$$\n",
    "\\overline m_{k+1}(d):= f(x^{k+1} + d).\n",
    "$$\n",
    "We already have by construction, \n",
    "$$\n",
    "\\tilde m_{k+1}(0)=\\overline m_{k+1}(0)=f(x^{k+1})\\qquad\\text{and}\\qquad \\nabla \\tilde m_{k+1}(0)=\\nabla \\overline m_{k+1}(0)=g(x^{k+1}).\n",
    "$$\n",
    "We enforce the new condition\n",
    "$$\n",
    "\\nabla m_{k+1}(-\\tau_k d^k)=\\nabla \\overline m_{k+1}(-\\tau_k d^k)=g^k.\n",
    "$$\n",
    "\n",
    "Noting $a^k:=g^{k+1}-g^k$ and $b^k:=\\tau^kd^k=x^{k+1}-x^k$, this is equivalent to $B^{k+1}b^k=a^k$. Assuming $B^{k+1}$ is invertible, this is equivalent to: $H^{k+1}$ solves\n",
    "\n",
    "$$\n",
    "\\tag{6}\n",
    "Ha^k=b^k.\n",
    "$$\n",
    "\n",
    "A necessary and sufficient condition for  (6) to admit a symmetric positive definite solution $H$ is the condition \n",
    "\n",
    "$$\n",
    "\\tag{7}\n",
    "\\left<a^k;b^k\\right> >0.\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We do not want to destroy all the information contained in $H^k$, so, assuming that (7) holds true, we choose a solution of (6) as close as possible of $H^k$. A popular choice is to set:\n",
    "$$\n",
    "\\tag{8}\n",
    "H^{k+1} := \\left(I-\\rho_k b^k\\otimes a^k\\right) H^k \\left(I-\\rho_k a^k\\otimes b^k\\right) + \\rho_k b^k\\otimes b^k,\\quad\\text{ with }\\quad \\rho_k:=\\dfrac1{\\left<a^k;b^k\\right>}.\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Question 16.__ Check that formula (8) provides a solution to (6). Check that $H^{k+1}$ defined this way is a symmetric positive definite matrix. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___Solution:___\n",
    "Assuming that $H^k$ is positive definite and that (7) is true, it is easy to check that $H^{k+1}$ is also positive definite. First, for $d\\in \\mathbb{R}^N$, noting $w=d-\\rho_k \\left<b^k;d\\right>a^k$, we compute,\n",
    "\n",
    "$$\n",
    " d^TH^{k+1}d =  w^TH^kw + \\rho_k  \\left<d;b^k\\right>^2\\ \\geq \\ 0.\n",
    "$$\n",
    "\n",
    "Next, using the same formula, we see that $d^TH^{k+1}d=0$ implies $w=0$ and $\\left<d;b^k\\right>=0$. The former implies that $d=\\lambda a^k$ for some $\\lambda\\in \\mathbb{R}$ and with the latter this yields $\\lambda\\left<a^k;b^k\\right>=0$. Hence $\\lambda=0$ and $d=0$. This proves that $H^{k+1}$ is positive definite.  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Question 17.__ Implement the BFGS method and apply it to the above function with $\\alpha = 0.1$, $\\beta=0.75$ and $x^0=(0,0)$. Start with $H^0=I$.\n",
    "\n",
    "Represent the iterations on a graph and plot $\\ \\log(f(x^k))\\ $ as a function of $k$. Observe and comment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Question 18.__ Does $H^k$ converge to  $[D^2 f(x^*)]^{-1}$?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Parameters\r\n",
    "alpha, beta = .1, .75\r\n",
    "epsilon = 1e-8\r\n",
    "itermax = 200\r\n",
    "iter_ls_max = 40\r\n",
    "\r\n",
    "##\r\n",
    "np.set_printoptions(precision=3)\r\n",
    "np.set_printoptions(suppress=\"True\")\r\n",
    "\r\n",
    "## initialization \r\n",
    "\r\n",
    "\r\n",
    "## Optmization loop\r\n",
    "\r\n",
    "\r\n",
    "# plot of the values of f along the iterations.\r\n",
    "\r\n",
    "\r\n",
    "# plot of the values of ||Happrox-Htrue|| along the iterations.\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('sdia-python': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "interpreter": {
   "hash": "47a3d19953d0682b2fed03a7de16340ad054781b6349bd7c1cab4a5c5105088a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}